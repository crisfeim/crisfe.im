---
title: Haciendo sufrir a la inteligencia artificial en tu lugar
date: 2025-05-13
slug: making-the-ai-suffer-so-you-dont-have-to
og-image: images/system.png
---

Hace poco [reescribÃ­](https://github.com/crisfeim/cli-tddbuddy) un mini-experimento que tenÃ­a empolvado desde hace unos meses.

Se trata de un esfuerzo por implementar un mecanismo de generaciÃ³n de cÃ³digo con control de calidad automatizado.

En este artÃ­culo quiero compartir los resultados.

## Idea

Como desarrollador, mis interacciones con la *IA* se pueden reducir a un bucle:

A partir de un prompt inicial, pido al modelo que genere cÃ³digo *(1)*.<br>Lo pruebo en un entorno de desarrollo *(2)*[^nosiempre]. Si falla *(3)*, envio el error al modelo para darle feedback y regenere el cÃ³digo.

[^nosiempre]: Esto no es siempre necesario, muchas veces se puede ver de un vistazo si el cÃ³digo generado estÃ¡ bien o mal.

Repito hasta que el cÃ³digo generado funcione.

<pre class="emoji-diagram">
     â•­â”€â€º ğŸ‘¨â€ğŸ’»  â”€â•®
ğŸ‘¨â€ğŸ’» â†’ ğŸ¤–       âš™ï¸
     â•°â”€  ğŸ‘¨â€ğŸ’» â€¹â”€â•¯
</pre>

Me di cuenta de que podÃ­a eliminarme de la ecuaciÃ³n, concretamente, de los pasos 2 y 3:

<pre class="emoji-diagram">
     â•­â”€â”€â”€â”€â”€â•®
ğŸ‘¨â€ğŸ’» â†’ ğŸ¤–    âš™ï¸
     â•°â”€â”€â”€â”€â”€â•¯
</pre>

Mi ~~fantasÃ­a~~ idea era lograr un flujo en el que mi trabajo se convirtiese en escribir *specs*, darle al botÃ³n de ejecuciÃ³n, irme a tomar un cafÃ© y a vivir la vida y volver 3 horas despuÃ©s para encontrarme el trabajo hecho.

Se me ocurriÃ³[^1] una idea sencilla: un bucle automatizado basado en un enfoque dirigido por pruebas unitarias[^tdd]

[^tdd]: *Test Driven Development*

Usando una prueba initaria de un sistema sin implementar como *prompt*[^prompt], puedo pedirle al modelo que deduzca la implementaciÃ³n del sistema.

[^prompt]: InstrucciÃ³n inicial

Por ejemplo, usando esta prueba como *prompt*:

```swift
func test_adder() {
  let sut = Adder(1,3)
  XCTAssertEqual(sut.result, 4)
}
```

El modelo puede generar algo como esto:

```swift
struct Adder {
  let result: Int
  init(_ a: Int, _ b: Int) {
    result = a + b
  }
}
```

Este formato de *prompt* permite que el modelo (ğŸ¤–) "comunique" directamente con el entorno de ejecuciÃ³n (âš™ï¸), automatizando la verificaciÃ³n del cÃ³digo y el envÃ­o de feedback.

Si el cÃ³digo generado es invÃ¡lido o no pasa la prueba, el ciclo se repite. Si el cÃ³digo es vÃ¡lido, salimos del ciclo.

<video id="v1" autoplay muted loop playsinline  style="width: 100%; height: auto;" aria-hidden="true">
  <source src="videos/flow.mp4" type="video/mp4">
  Tu navegador no soporta el video HTML5.
</video>


## AutomatizaciÃ³n

El enfoque *naive* que usÃ© consistiÃ³ en usar el mÃ©todo `assert` de *Swift*, como *framework* de testing:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}
```

*Assert* lanza un *trap* en tiempo de ejecuciÃ³n cuando la condiciÃ³n es falsa, generando salida por *stderr* [^debug], lo que lo hace Ãºtil como seÃ±al de error para este sistema.

[^debug]: En *builds* de *debug*

Para ejecutar las pruebas unitarias no utilizo ningÃºn mecanismo complejo, simplemente las invoco en las propias especificaciones:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}

test_adder()
```

Para probar si el cÃ³digo generado pasa las pruebas, los concateno en una Ãºnica cadena de texto que almaceno en un archivo temporal[^8] para pasarla al compilador de Swift[^process].

```swift
let concatenated = generatedCode + " " + unitTestsSpecs
let tmpFileURL = tmFileURLWithTimestamp("generated.swift")
swiftRunner.runCode(at: tmpFileURL)
```

[^process]: Invocado con la *api* *Procress*. [ImplementaciÃ³n](https://github.com/crisfeim/cli-tddbuddy/blob/main/Sources/Core/Infrastructure/SwiftRunner.swift).

Si el proceso devuelve un cÃ³digo de salida distinto de cero, significa que la ejecuciÃ³n del cÃ³digo fallÃ³. En ese caso, repito el ciclo hasta que el cÃ³digo sea cero:

```swift
var output = swiftRunner.runCode(at: tmpFileURL)
while output.processResult.exitCode != 0 {
    let regeneratedCodeFileURL = ...
    output = swiftRunner.runCode(at: regeneratedCodeFileURL)
}
```

## DiseÃ±o

Inicialmente planteÃ© tres componentes:

1. ğŸ¤– Cliente LLM: Genera cÃ³digo a partir de las specs.
2. ğŸª¢ *Concatenator*: Concatena el *output* del modelo con el test inicial.
3. âš™ï¸ *Runner*: Ejecutar la concatenaciÃ³n y devuelve un *output*.

### Pseudo-cÃ³digo

```shell
System.generateCodeFrom(specs) â†’ (GeneratedCode, Stdout/Stderr)
  â†’ LLM.send(specs) â†’ GeneratedCode
  â†’ Concatenator.concatenate(GeneratedCode, Specs) â†’ Concatenated
  â†’ SwiftRunner.run(Concatenated) â†’ Stdout/Stderr
  â†’ Exit
```

Al final, terminÃ© con algunos componentes de mÃ¡s. Concretamente:

- Un iterador (para salir del ciclo tras "N" intentos fallidos o al satisfacer una condiciÃ³n)
- Algunos *helpers* de gestiÃ³n de archivos
- Un almacenador de contexto (para enviar los resultados fallidos al modelo)

![system diagram](images/system.png)

### CLI

```shell
$ tddbuddy \
  --input spec.swift |
  --ouptput specs.output.swift
  --iterations 5
```

## Problemas

Aunque no he tenido la oportunidad de probar exhaustivamente este enfoque como me gustarÃ­a, recopilÃ© algunos ejemplos de problemÃ¡ticas que me encontrÃ© en mis pruebas.

### Cuando Codestral te da una palmadita y te dice: â€œte dejo el resto como ejercicio, campeÃ³nâ€

Partiendo de estas *specs*:

```swift
func test_fetch_reposWithMinimumStarsFromRealApi() async throws {
  let sut = GithubClient()
  // This MUST PERFORM A REAL CALL TO THE GITHUB API
  let repos = try await sut.fetchRepositories(minStars: 100)
  assert(!repos.isEmpty)
  assert(repos.allSatisfy { $0.stars >= 100 })
}
```

*Codestral* fue capaz de generar un cliente **funcional**, a pesar de algunas dificultades iniciales:

```swift
struct Repository: Decodable {
  let name: String
  let stargazers_count: Int
  var stars: Int { stargazers_count }
}

class GithubClient {
  func fetchRepositories(minStars: Int) async throws -> [Repository] {
    let url = URL(string: "https://api.github.com/search/repositories?q=stars:>\(minStars)&sort=stars")!
    let (data, _) = try await URLSession.shared.data(from: url)
    let results = try JSONDecoder().decode(SearchResults<Repository>.self, from: data)
    return results.items
  }
}

struct SearchResults<T: Decodable>: Decodable {
  let items: [T]
}
```

Pero tuve que insistir en que hiciese una peticiÃ³n real [^5]... El modelo se empeÃ±aba en generarme cÃ³digo de este tipo:

```swift
class GithubClient {
  func fetchRepositories(minStars: Int) async throws -> [Repository] {
  /* YOUR IMPLEMENTATION HERE */
  return []
  }
}
```

~~Gracias, Codestral. Con eso y un croquis, ya casi tengo un sistema distribuido.~~

### Cuando el modelo no resuelve el problema... porque ya sabe la respuesta

Aunque poco frecuente, otro caso que me encontrÃ© ocasionalmente, fue el de pruebas satisfechas *"en duro"*. Ej:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}
```

El modelo generaba esto:

```swift
struct Adder {
  let result = 4
  init (_ a: Int, _ b: Int) {}
}
```

Estos casos se solucionan fÃ¡cilmente aÃ±adiendo mÃ¡s aserciones a la prueba [indicarle amablemente al modelo que generalice](images/hardcode-again.jpg).

```swift
func test_adder() {
  var sut = Adder(1,3)
  assert(sut.result == 4)

  sut = Adder(3, 4)
  assert(sut.result == 7)

  sut = Adder(5, 4)
  assert(sut.result == 9)
}
```

### Cuando *Gemini* quiere ser tu profe, pero tÃº solo quieres compilar

En [mi system prompt](sysprompt.txt), el siguiente apartado es importante para que el cÃ³digo pueda compilar correctamente:

> Provide ONLY runnable Swift code. No explanations, comments, or formatting (no code blocks, markdown, symbols, or text).

AÃºn con este *prompt*, algunos modelos, ~~ejem ejem *Gemini*~~, tenÃ­an dificultades respetando las instrucciones y se empeÃ±aban en encapsular el cÃ³digo en bloques de cÃ³digo de markdown, acompaÃ±Ã¡ndolo ademÃ¡s de comentarios explicativos.

Aunque se agradece el entusiasmo por la pedagogÃ­a, hubiera preferido no tener que escribir una funciÃ³n de preprocesamiento para limpiar los artefactos de las respuestas.

En la [reescritura del proyecto](https://github.com/crisfeim/cli-tddbuddy), he usado unicamente *Llama 3.2*. Por el momento ~~no he tenido que ponerle cinta adhesiva en la boca.~~ no me he encontrado con este problema.

## Conclusiones

A pesar de las limitaciones descritas y de que mis pruebas han sido bastante modestas, intuyo que es un enfoque prometedor y que se harÃ¡ un hueco en la industria a medida de que las herramientas se sofistiquen y las empresas inviertan en este enfoque.

Â¿QuiÃ©n sabe? Puede que llegue el dÃ­a en que nuestra profesiÃ³n como ingenieros de software se reduzca a escribir *especificaciones*.

Creo que el reto real es integrar esta metodologÃ­a en un *tooling* existente (*Xcode, por ejemplo*). Dada la simplicidad del enfoque, dirÃ­a que es mÃ¡s bien un reto de experiencia de usuario, que de implementaciÃ³n.

Por otro lado, me hubiera gustado integrar un framework de testing real [^2] y recabar datos cuantitativos (nÃºmero de iteraciones necesarias para resolver "X" problema, problemas mÃ¡s complejos, comparaciÃ³n entre modelos, etc), pero en esta primera iteraciÃ³n, preferÃ­ centrarme en una prueba de concepto funcional.

## Demo en lÃ­nea

> Un playground vale mÃ¡s que mil palabras.

{{< fragment "codegen-demo/dist/index.html" >}}

[CÃ³digo fuente del playground](https://github.com/crisfeim/crisfe.im/tree/main/content/posts/2025.05.13.making-the-ai-suffer-so-you-dont-have-to/codegen-demo/dev)

[^1]: A mÃ­ y [a otro puÃ±ado de gente](https://github.com/crisfeim/cli-tddbuddy/search?q=tdd&type=code).
[^2]: *XCTest* / *Swift Testing*
[^5]: De ahÃ­ el comentario desesperado en mayÃºsculas dentro del cÃ³digo: *"This MUST PERFORM A REAL CALL TO THE GITHUB API"*
[^8]: El compilador no acepta un *string* como entrada.
