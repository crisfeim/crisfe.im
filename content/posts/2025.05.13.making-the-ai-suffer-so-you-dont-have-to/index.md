---
title: Making the AI suffer so you don't have to
date: 2025-05-13
slug: making-the-ai-suffer-so-you-dont-have-to
---

Hace unos meses escribÃ­ un mini-experimento para generar cÃ³digo a partir de unit tests con inteligencia artificial.

Al final lo tuve empolvado mucho tiempo, hasta que hace poco lo retomÃ© [rehaciÃ©ndolo desde cero](https://github.com/crisfeim/cli-tddbuddy).

En este artÃ­culo quiero compartir los resultados.

{{< toc >}}

## Idea

Como desarrollador, mis interacciones con la *IA* se pueden reducir a un bucle:

A partir de un prompt inicial, pido al modelo que genere cÃ³digo *(1)*.<br>Lo pruebo en un entorno de dev *(2)*[^nosiempre]. Si falla *(3)*, doy feedback al modelo.

[^nosiempre]: Esto no es siempre necesario, la mayorÃ­a de las veces puedes ver de un vistazo la validez del cÃ³digo.

Repito hasta que el cÃ³digo generado funcione.

<pre style="font-size: 1.5rem">
     â•­â”€ ğŸ‘¨â€ğŸ’» â”€â•®
ğŸ‘¨â€ğŸ’» â†’ ğŸ¤–     âš™ï¸
     â•°â”€ ğŸ‘¨â€ğŸ’» â”€â•¯
</pre>

Me di cuenta de que podÃ­a eliminarme de la ecuaciÃ³n, concretamente, de los pasos 2 y 3:

<pre style="font-size: 1.5rem">
     â•­â”€â”€â”€â”€â”€â•®
ğŸ‘¨â€ğŸ’» â†’ ğŸ¤–    âš™ï¸
     â•°â”€â”€â”€â”€â”€â•¯
</pre>

Mi fantasÃ­a era lograr un flujo en el que mi trabajo se convirtiese en escribir *specs*, darle al botÃ³n de ejecuciÃ³n, irme a tomar un cafÃ© y a vivir la vida y volver 3 horas despuÃ©s para encontrarme el trabajo hecho.

Se me ocurriÃ³[^1] una idea sencilla: un bucle automatizado basado en un enfoque *test driven*.

Usando una prueba initaria como *prompt*, puedo pedirle al modelo que infiera la implementaciÃ³n del *SUT* sin implementar.

Por ejemplo, usando este test como prompt:

```swift
func test_adder() {
  let sut = Adder(1,3)
  XCTAssertEqual(sut.result, 4)
}
```

El modelo puede generar algo como esto:

```swift
struct Adder {
  let result: Int
  init(_ a: Int, _ b: Int) {
    result = a + b
  }
}
```

Este formato de *prompt* permite que el modelo (ğŸ¤–) se "hable" directamente con el entorno de ejecuciÃ³n (âš™ï¸), automatizando la verificaciÃ³n del cÃ³digo y el envÃ­o de feedback.

Si la compilaciÃ³n o el test fallan, el ciclo se repite. Si no, podemos salir del bucle:

<video id="v1" autoplay muted loop playsinline  style="width: 100%; height: auto;">
  <source src="flow.mp4" type="video/mp4">
  Tu navegador no soporta el video HTML5.
</video>


## AutomatizaciÃ³n

El enfoque *naive* que usÃ© consistiÃ³ en usar el mÃ©todo `assert` de *Swift*, como *framework* de testing. Es decir:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}
```

*Assert* lanza un trap en tiempo de ejecuciÃ³n cuando la condiciÃ³n es falsa, generando salida por *stderr* [^debug], lo que lo hace Ãºtil como seÃ±al de error para este sistema.

[^debug]: En *builds* de *debug*

Para invocar los tests unitarios tampoco utilizo algÃºn mecanismo complejo de anÃ¡lisis de sÃ­ntaxis, parsing, AST, etc..., simplemente los llamo en la propia *spec*:

```swift
func test_adder() { ... }
func test_substractor() { ... }
...
test_adder()
test_substractor()
```

Para verificar el cÃ³digo generado contra su test, concateno ambos en una Ãºnica cadenal que almaceno en un archivo temporal[^8] y paso al compilador[^process].

```swift
let concatenated = generatedCode + " " + unitTestsSpecs
let tmpFileURL = tmFileURLWithTimestamp("generated.swift")
swiftRunner.runCode(at: tmpFileURL)
```

[^process]: Invocado con `Procress`. [ImplementaciÃ³n](https://github.com/crisfeim/cli-tddbuddy/blob/main/Sources/Core/Infrastructure/SwiftRunner.swift).

Si el *exit code* es distinto de cero, significa que la ejecuciÃ³n del cÃ³digo generado fallÃ³. En ese caso, repetimos el ciclo hasta que el cÃ³digo sea cero.

```swift
var output = swiftRunner.runCode(at: tmpFileURL)
while output.processResult.exitCode != 0 {
    let regeneratedCodeFileURL = ...
    output = swiftRunner.runCode(at: regeneratedCodeFileURL)
}
```

## DiseÃ±o

Inicialmente planteÃ© tres componentes:

1. ğŸ¤– Cliente LLM: Genera cÃ³digo a partir de las specs.
2. ğŸª¢ *Concatenator*: Concatena el *output* del modelo con el test inicial.
3. âš™ï¸ *Runner*: Ejecutar la concatenaciÃ³n y devuelve un *output*.

### Pseudo-cÃ³digo

```
System.generateCodeFrom(specs) â†’ (GeneratedCode, Stdout/Stderr)
  â†’ LLM.send(specs) â†’ GeneratedCode
  â†’ Concatenator.concatenate(GeneratedCode, Specs) â†’ Concatenated
  â†’ SwiftRunner.run(Concatenated) â†’ Stdout/Stderr
  â†’ Exit
```

Aunque terminÃ© con algunos componentes de mÃ¡s. Concretamente, un iterador (para cortar el bucle tras "N" intentos fallidos), *helpers* de gestiÃ³n de archivos y un almacenador de contexto:

<a href="system.png"><img src="system.png" alt="system diagram"></a>

## Uso

```shell
$ tddbuddy \
  --input spec.swift |
  --ouptput specs.output.swift
  --iterations 5
```

## Limitaciones

Aunque no he tenido la oportunidad de probar exhaustivamente este enfoque, aquÃ­ hay algunos ejemplos y limitaciones que me encontrÃ© en el camino.

### Cuando Codestral te da una palmadita y te dice: â€œte dejo el resto como ejercicioâ€

Partiendo de estas *specs*:

```swift
func test_fetch_repositories_with_minimum_stars_from_real_api() async throws {
  let sut = GithubClient()
  // This MUST PERFORM A REAL CALL TO THE GITHUB API
  let repos = try await sut.fetchRepositories(minStars: 100)
  if repos.count == 0 {
  fail("List should not be empty")
  }

  if !repos.allSatisfy { $0.stars >= 100 } {
    fail("Fetched repositories should have at least 100 hundred stars")
  }
}

try await test_fetch_repositories_with_minimum_stars_from_real_api()

func fail(_ description: String, function: String = #function) {
  print("âŒ â€” \(function), \(description)")
}
```

*Codestral* fue capaz de generar un cliente **funcional**, a pesar de algunas dificultades iniciales:

```swift
struct Repository: Decodable {
  let name: String
  let stargazers_count: Int
  var stars: Int { stargazers_count }
}

class GithubClient {
  func fetchRepositories(minStars: Int) async throws -> [Repository] {
    let url = URL(string: "https://api.github.com/search/repositories?q=stars:>\(minStars)&sort=stars")!
    let (data, _) = try await URLSession.shared.data(from: url)
    let results = try JSONDecoder().decode(SearchResults<Repository>.self, from: data)
    return results.items
  }
}

struct SearchResults<T: Decodable>: Decodable {
  let items: [T]
}
```

Tuve que insistir en que hiciese una peticiÃ³n real [^5], porque el modelo se empeÃ±aba en generarme cÃ³digo de este tipo:

```swift
class GithubClient {
  func fetchRepositories(minStars: Int) async throws -> [Repository] {
  /* YOUR IMPLEMENTATION HERE */
  return []
  }
}
```

~~Gracias, Codestral. Con eso y un croquis, ya casi tengo un sistema distribuido.~~

### Cuando el modelo no resuelve el problema... porque ya sabe la respuesta

Aunque poco frecuente, otro caso que me encontrÃ© ocasionalmente, fue el de *resultados hardcodeados*. Ej:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}
```

El modelo generaba esto:

```swift
struct Adder {
  let result = 4
  init (_ a: Int, _ b: Int) {}
}
```

Estos casos solucionan fÃ¡cilmente aÃ±adiendo mÃ¡s aserciones al test [para obligar al modelo a generalizar.](hardcode-again.jpg)

```swift
func test_adder() {
  var sut = Adder(1,3)
  assert(sut.result == 4)

  sut = Adder(3, 4)
  assert(sut.result == 7)

  sut = Adder(5, 4)
  assert(sut.result == 9)
}
```

### Cuando el modelo quiere ser tu profe, pero tÃº solo quieres compilar

En [mi system prompt](system-prompt.txt), el siguiente apartado es importante para que el cÃ³digo pueda compilar correctamente:

> Provide ONLY runnable Swift code. No explanations, comments, or formatting (no code blocks, markdown, symbols, or text).

AÃºn con este *prompt*, algunos modelos, ~~ejem ejem *Codestral*~~, tenÃ­an dificultades entendiendo el contexto y se empeÃ±aban en encapsular el cÃ³digo en bloques de cÃ³digo de markdown, acompaÃ±Ã¡ndolo ademÃ¡s de comentarios explicativos.

Y aunque se agradece el entusiasmo por la pedagogÃ­a, hubiera preferido no tener que escribir una funciÃ³n de preprocessing para limpiar artefactos de output.

En la [reescritura del proyecto](https://github.com/crisfeim/cli-tddbuddy), he usado sÃ³lo *Llama 3.2*. Por el momento ~~no he tenido que ponerle cinta adhesiva en la boca.~~ no me he encontrado con este problema.

## Conclusiones

A pesar de las limitaciones descritas y de que mis pruebas han sido bastante modestas, intuyo que es un enfoque prometedor y que se harÃ¡ un hueco en la industria a medida de que las herramientas se sofistiquen y las empresas inviertan en este enfoque.

Â¿QuiÃ©n sabe? Puede que llegue el dÃ­a en que nuestra profesiÃ³n como ingenieros de software se reduzca a escribir *specs*.

Creo que el reto real es integrar esta metodologÃ­a en un *tooling* existente (*Xcode, por ejemplo*). Dada la simplicidad del enfoque, dirÃ­a que es mÃ¡s bien un reto de experiencia de usuario, que de implementaciÃ³n.

Por otro lado, me hubiera gustado integrar un framework de testing real [^2] y recabar datos cuantitativos (nÃºmero de iteraciones necesarias para resolver "X" problema, problemas mÃ¡s complejos, comparaciÃ³n entre modelos, etc), pero en esta primera iteraciÃ³n, preferÃ­ centrarme en una prueba de concepto funcional.

## Demo en lÃ­nea

> Un playground vale mÃ¡s que mil palabras.

[CÃ³digo fuente del playground](https://github.com/crisfeim/app-web-tddbuddy)

[^1]: A mÃ­ y [a otro puÃ±ado de gente](https://github.com/crisfeim/cli-tddbuddy/search?q=tdd&type=code).
[^2]: *XCTest* / *Swift Testing*
[^5]: De ahÃ­ el comentario desesperado en mayÃºsculas dentro del cÃ³digo: *"This MUST PERFORM A REAL CALL TO THE GITHUB API"*
[^8]: El compilador no acepta un *string* como entrada.
