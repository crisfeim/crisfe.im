---
title: Making the AI suffer so you don't have to
date: 2025-05-13
slug: making-the-ai-suffer-so-you-dont-have-to
---

Hace unos meses escribÃ­ un mini-experimento para generar cÃ³digo a partir de unit tests con inteligencia artificial.

Al final lo tuve empolvado mucho tiempo, hasta que hace poco lo retomÃ© [rehaciÃ©ndolo desde cero](https://github.com/crisfeim/cli-tddbuddy).

En este artÃ­culo quiero compartir los resultados.

{{< toc >}}

## Idea

Mis interacciones con la *IA* se pueden reducir siguiente un bucle:

A partir de un prompt inicial *(1)*, el modelo genera cÃ³digo *(2)* que pruebo en un entorno de dev *(3)*.

Si falla *(4)*, envÃ­o ese *output* al modelo para que regenere cÃ³digo (5).

Repito el proceso hasta que el cÃ³digo funcione.

```
ğŸ‘¨â€ğŸ’» â†’ ğŸ¤–
ğŸ¤– â†’ ğŸ‘¨â€ğŸ’»
ğŸ‘¨â€ğŸ’» â†’ âš™ï¸
âš™ï¸ â†’ ğŸ”´
 â†ªï¸ Repeat
```

El prompt inicial sigue siendo el mismo, lo Ãºnico que cambia es la adiciÃ³n del *output* del paso *4*, que uso como feedback para regenerar cÃ³digo.

Me vino la idea a la cabeza (y las ganas) de eliminarme de la ecuaciÃ³n, concretamente, de los pasos 2 y 3:


<video id="v2" autoplay muted loop playsinline  style="width: 100%; height: auto;">
  <source src="video.mov" type="video/mp4">
  Tu navegador no soporta el video HTML5.
</video>

El sueÃ±o era lograr un flujo en el que mi trabajo se convirtiese en escribir *specs*, darle al botÃ³n de ejecuciÃ³n, irme a tomar un cafÃ© y volver 3 horas despuÃ©s para encontrarme el trabajo hecho.

Para ello, se me ocurriÃ³[^1] una idea sencilla: un bucle automatizado basado en un enfoque TDD.

A partir de una prueba unitaria inicial, cuyo sistema sea un *SUT* inexistente, pedirÃ­a al modelo la generaciÃ³n de ese *SUT*.

Para que quede mÃ¡s claro, este es un ejemplo sencillo de *spec*:

```swift
func test_adder() {
  let sut = Adder(1,3)
  XCTAssertEqual(sut.result, 4)
}
```

Si le pedimos al modelo que implemente el *SUT*, querremos que nos devuelva:

```swift
struct Adder {
  let result: Int
  init(_ a: Int, _ b: Int) {
    result = a + b
  }
}
```

Usar una prueba unitaria como prompt permite que el modelo (ğŸ¤–) se conecte directamente con el entorno de ejecuciÃ³n (âš™ï¸), automatizando la verificaciÃ³n del cÃ³digo y el envÃ­o de feedback.

Si la compilaciÃ³n o el test fallan, el ciclo se repite. Si no, rompemos el bucle:

<video id="v1" autoplay muted loop playsinline  style="width: 100%; height: auto;">
  <source src="flow.mp4" type="video/mp4">
  Tu navegador no soporta el video HTML5.
</video>


## EjecuciÃ³n de cÃ³digo

Para mi primer prototipo, usÃ© Swift y un enfoque *naive* que consistiÃ³ en usar el mÃ©todo `assert(Bool)` como *framework* de testing[^3].

Para ejecutar el cÃ³digo generado con su test, simplemente los concatenamos en una Ãºnica cadena y se la pasamos al compilador:

```bash
// Ejemplo de una posible implementaciÃ³n
let concatenated = generatedCode + " " + unitTestsSpecs
let tmpFileURL = tmFileURLWithTimestamp("generated.swift")
swiftRunner.runCode(at: tmpFileURL)
```

El cÃ³digo del *runner* estÃ¡ disponible [aquÃ­](@todo)

## DiseÃ±o y componentes del sistema

Para este sistema, inicialmente me planteÃ© tres componentes:

1. ğŸ¤– Cliente LLM: Genera cÃ³digo a partir de las specs.
2. ğŸª¢ *Concatenator*: Concatena el *output* del modelo con el test inicial.
3. âš™ï¸ *Runner*: Ejecutar la concatenaciÃ³n y devuelve un *output*.

### Pseudo-cÃ³digo

```
System.generateCodeFrom(specs) â†’ (GeneratedCode, Stdout/Stderr)
  â†’ LLM.send(specs) â†’ GeneratedCode
  â†’ Concatenator.concatenate(GeneratedCode, Specs) â†’ Concatenated
  â†’ SwiftRunner.run(Concatenated) â†’ Stdout/Stderr
  â†’ Exit
```

Al final terminÃ© con algunos componentes de mÃ¡s. Concretamente, un iterador (para romper el bucle despuÃ©s de "N" iteraciones) y *helpers* de gestiÃ³n de archivos:

<a href="system.png"><img src="system.png" alt="system diagram"></a>

## Limitaciones

Aunque no he tenido la oportunidad de probar exhaustivamente este enfoque, aquÃ­ hay algunos ejemplos y limitaciones que me encontrÃ© en el camino.

### Cuando Codestral te da una palmadita y te dice: â€œte dejo el resto como ejercicioâ€

Partiendo de estas *specs*:

```swift
func test_fetch_repositories_with_minimum_stars_from_real_api() async throws {
  let sut = GithubClient()
  // This MUST PERFORM A REAL CALL TO THE GITHUB API
  let repos = try await sut.fetchRepositories(minStars: 100)
  if repos.count == 0 {
  fail("List should not be empty")
  }

  if !repos.allSatisfy { $0.stars >= 100 } {
    fail("Fetched repositories should have at least 100 hundred stars")
  }
}

try await test_fetch_repositories_with_minimum_stars_from_real_api()

func fail(_ description: String, function: String = #function) {
  print("âŒ â€” \(function), \(description)")
}
```

A pesar de algunas dificultades iniciales, *Codestral* fue capaz de generar un cliente **funcional**:

```swift
struct Repository: Decodable {
  let name: String
  let stargazers_count: Int
  var stars: Int { stargazers_count }
}

class GithubClient {
  func fetchRepositories(minStars: Int) async throws -> [Repository] {
    let url = URL(string: "https://api.github.com/search/repositories?q=stars:>\(minStars)&sort=stars")!
    let (data, _) = try await URLSession.shared.data(from: url)
    let results = try JSONDecoder().decode(SearchResults<Repository>.self, from: data)
    return results.items
  }
}

struct SearchResults<T: Decodable>: Decodable {
  let items: [T]
}
```

Tuve que insistir en que hiciese una peticiÃ³n real [^5], porque el modelo se empeÃ±aba en generarme cosas de este tipo:

```swift
class GithubClient {
  func fetchRepositories(minStars: Int) async throws -> [Repository] {
/* YOUR IMPLEMENTATION HERE */
  return ...
  }
}
```

~~Gracias, Codestral. Con eso y un croquis, ya casi tengo un sistema distribuido.~~

### Cuando el modelo no resuelve el problema... porque ya sabe la respuesta

Aunque poco frecuente, otro caso que me encontrÃ© ocasionalmente, fue el de *outputs hardcodeados*. Ej:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}
```

El modelo generaba esto:

```swift
struct Adder {
  let result = 4
  init (_ a: Int, _ b: Int) {}
}
```

Esto se soluciona fÃ¡cilmente aÃ±adiendo mÃ¡s aserciones al test, [para obligar al modelo a generalizar.](hardcode-again.jpg)

```swift
func test_adder() {
  var sut = Adder(1,3)
  assert(sut.result == 4)

  sut = Adder(3, 4)
  assert(sut.result == 7)

  sut = Adder(5, 4)
  assert(sut.result == 9)
}
```

### Cuando el modelo quiere ser tu profe, pero tÃº solo quieres compilar

En [mi system prompt](system-prompt.txt), el siguiente apartado es importante para que el cÃ³digo pueda compilar correctamente:

> Provide ONLY runnable Swift code. No explanations, comments, or formatting (no code blocks, markdown, symbols, or text).

Algunos modelos, ~ejem ejem *Codestral*~, tenÃ­an dificultades entendiendo el contexto y se empeÃ±aban en encapsular el cÃ³digo en bloques de cÃ³digo de markdown, acompaÃ±Ã¡ndolo ademÃ¡s de comentarios explicativos.

Y aunque se agradece el entusiasmo por la pedagogÃ­a, hubiera preferido que no me obligara a escribir una funciÃ³n de preprocessing para limpiar los artefactos de su output.

En la [reescritura del proyecto](https://github.com/crisfeim/cli-tddbuddy), he usado sÃ³lo *Llama 3.2*. Por el momento ~~no he tenido que ponerle cinta adhesiva en la boca.~~ no me he encontrado con este problema.

## Conclusiones

A pesar de las limitaciones descritas y de que mis pruebas han sido bastante modestas, intuyo que es un enfoque prometedor y que se harÃ¡ un hueco en la industria a medida de que las herramientas se sofistiquen y las empresas inviertan en este enfoque.

Â¿QuiÃ©n sabe? Puede que algÃºn dÃ­a nuestro cotidiano como ingenieros de software se reduzca a escribir *specs*.

Creo que el reto real es integrar esta metodologÃ­a en un *tooling* existente (*Xcode, por ejemplo*). Dada la simplicidad del enfoque, dirÃ­a que es mÃ¡s bien un reto de experiencia de usuario, que de implementaciÃ³n.

Por otro lado, me hubiera gustado integrar un framework de testing real [^2] y recabar datos cuantitativos (nÃºmero de iteraciones necesarias para resolver "X" problema, problemas mÃ¡s complejos, comparaciÃ³n entre modelos, etc), pero preferÃ­ centrarme primero en tener una prueba de concepto funcional. Queda como tarea pendiente.

## Demo en lÃ­nea

> Un playground vale mÃ¡s que mil palabras.

[CÃ³digo fuente del playground](https://github.com/crisfeim/app-web-tddbuddy)

[^1]: A mÃ­ y [a otro puÃ±ado de gente](https://github.com/crisfeim/cli-tddbuddy/search?q=tdd&type=code).
[^2]: *XCTest* / *Swift Testing*
[^3]: El mÃ©todo assert lanza un trap en tiempo de ejecuciÃ³n cuando la condiciÃ³n es falsa, generando salida por *stderr* (en *builds* de *debug*), lo que lo hace Ãºtil como seÃ±al de error para este sistema
[^5]: De ahÃ­ el comentario desesperado en mayÃºsculas dentro del cÃ³digo: *"This MUST PERFORM A REAL CALL TO THE GITHUB API"*
[^8]: El compilador no acepta un *string* como entrada.
