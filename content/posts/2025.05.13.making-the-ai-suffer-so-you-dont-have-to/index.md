---
title: "Test-Driven Prompting: Making the AI Write Code While You Make Coffee"
date: 2025-05-13
slug: making-the-ai-suffer-so-you-dont-have-to
og-image: images/system.png
---

### tldr;

C√≥mo construir un sistema en el que la *IA* genera c√≥digo a partir de especificaciones de pruebas unitarias, lo compila, lo ejecuta y si falla lo vuelve a intentar hasta que funciona, sin intervenci√≥n humana.

## Introducci√≥n

¬øQu√© pasa si delegas el trabajo aburrido a una IA?

Ese fue el punto de partida de este experimento: construir un sistema que no solo genere c√≥digo autom√°ticamente a partir de especificaciones, sino que lo compile, lo pruebe, y lo repita hasta que acierte ‚Äî sin intervenci√≥n humana.

La idea era convertir el rol del desarrollador en escribir pruebas, darle al bot√≥n de ejecutar, y desaparecer. Si el modelo se equivoca, que se corrija solo. Si se cae, que se levante. Si se rinde‚Ä¶ pues que no se rinda.

En este art√≠culo te cuento c√≥mo mont√© este sistema de automatizaci√≥n con feedback en bucle y qu√© aprend√≠ en el proceso.

## Idea

Como desarrollador, mis interacciones con la *IA* se pueden reducir a un bucle:

*(1)* A partir de un prompt inicial, pido al modelo que genere c√≥digo.<br>
*(2)* Lo pruebo en un entorno de desarrollo<br>
*(3)* Si falla, envio el error al modelo para darle feedback y vuelva a regenerar el c√≥digo.

Repito el ciclo hasta que el c√≥digo generado funcione.

Me di cuenta de que pod√≠a eliminarme de la ecuaci√≥n, concretamente, de los pasos *(2)* y *(3)*:


<video id="v1" autoplay muted loop playsinline style="width:45%;" aria-hidden="true">
  <source src="videos/loop.mov" type="video/mp4">
</video>

Mi ~~fantas√≠a~~ idea era lograr un flujo en el que mi trabajo se convirtiese en escribir *specs*, darle al bot√≥n de ejecuci√≥n, irme a tomar un caf√© y a vivir la vida para volver 3 horas despu√©s y encontrarme con el trabajo hecho.

Se me ocurri√≥ una idea sencilla[^1]: un bucle automatizado basado en un enfoque dirigido por pruebas unitarias.[^tdd]

[^tdd]: *Test Driven Development*

Si usamos como *prompt* una prueba de un sistema sin implementaci√≥n, podemos pedirle al modelo que la deduzca a partir de las aserciones de la prueba.

Por ejemplo, esto es suficientemente expl√≠cito para que el modelo entienda lo que queremos:

```swift
func test_adder() {
  let sut = Adder(1,3)
  XCTAssertEqual(sut.result, 4)
}
```

A partir de esa prueba unitaria como *prompt*, podr√° generar cualquier variante de c√≥digo que satisfaga las aserciones:

```swift
struct Adder {
  let result: Int
  init(_ a: Int, _ b: Int) {
    result = a + b
  }
}
```

Este formato de *prompt* permite que el modelo (ü§ñ) pueda "comunicar" directamente con el entorno de ejecuci√≥n (‚öôÔ∏è), automatizando la verificaci√≥n de la validez del c√≥digo y el bucle de retroalimentaci√≥n.

Si el c√≥digo generado es inv√°lido o no pasa la prueba, el ciclo se repite. Si el c√≥digo es v√°lido, salimos del ciclo.

<video id="v1" autoplay muted loop playsinline  style="width: 100%; height: auto;" aria-hidden="true">
  <source src="videos/flow.mp4" type="video/mp4">
</video>

### Prompt

Este es el *prompt* que utilic√© en mis pruebas. Seguramente mejorable, pero funcion√≥ para el experimento:

> Imagine that you are a programmer and the user's responses are feedback from compiling your code in your development environment. Your responses are the code you write, and the user's responses represent the feedback, including any errors.
>
> Implement the SUT's code in Swift based on the provided specs (unit tests).
>
>Follow these strict guidelines:
>
> 1. Provide ONLY runnable Swift code. No explanations, comments, or formatting (no code blocks, markdown, symbols, or text).
> 2. DO NOT include unit tests or any test-related code.
> 3. ALWAYS IMPORT ONLY Foundation. No other imports are allowed.
> 4. DO NOT use access control keywords (`public`, `private`, `internal`) or control flow keywords in your constructs.
>
> If your code fails to compile, the user will provide the error output for you to make adjustments.

## Automatizaci√≥n

El enfoque *naive* que us√© para ejecutar el c√≥digo generado contra las pruebas consisti√≥ en usar el m√©todo `assert` de *Swift* como *framework* de testing[^xctest]:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}
```
[^xctest]: Hasta donde s√©, lanzar *XCTest* de forma *standalone* es bastante complicado, y mi intenci√≥n era tener una prueba de concepto funcional sin mayor complicaci√≥n.

*Assert* lanza un *trap* en tiempo de ejecuci√≥n cuando la condici√≥n es falsa, generando salida por *stderr*, lo que lo hace √∫til como se√±al de error para este sistema.


Para ejecutar las pruebas unitarias simplemente las invocamos de forma manual en las propias especificaciones:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}

test_adder()
```

Concatenamos el c√≥digo generado y las pruebas unitarias en una √∫nica cadena de texto que almacenamos en un archivo temporal[^8] y se lo pasamos al compilador, en este caso, Swift[^process].


```swift
let concatenated = generatedCode + "\n" + unitTestsSpecs
let tmpFileURL = tmFileURLWithTimestamp("generated.swift")
swiftRunner.runCode(at: tmpFileURL)
```

[^process]: Invocado con la *api* *Process*. [Implementaci√≥n](https://github.com/crisfeim/cli-tddbuddy/blob/main/Sources/Core/Infrastructure/SwiftRunner.swift).

Si el proceso devuelve un c√≥digo de salida distinto de cero, significa que la ejecuci√≥n del c√≥digo fall√≥. En ese caso, repetimos el ciclo hasta que el c√≥digo sea cero:

```swift
var output = swiftRunner.runCode(at: tmpFileURL)
while output.processResult.exitCode != 0 {
    let regeneratedCodeFileURL = ...
    output = swiftRunner.runCode(at: regeneratedCodeFileURL)
}
```

## Demo en l√≠nea

> ‚Ñπ  Escribe a la izquierda las especificaciones y dale al bot√≥n *play*.

Puedes usar `assertEqual` como mini-framework de testing o escribe tus propios m√©todos en la propia *textarea*.

Puedes usar *GPT3.5* cortes√≠a de *llm7*, *Gemini* (requiere clave) o *Llama3.2* [^llama]

[^llama]: Tendr√°s que [descargar el *index.html* de la demo](demo) y servirlo desde un servidor local.

{{< fragment "demo/index.html" >}}


## Dise√±o

Estos son los principales componentes del sistema:

1. ü§ñ *Client*: Genera c√≥digo a partir de unas specs.
2. ü™¢ *Concatenator*: Concatena el *output* del modelo con el test inicial.
3. ‚öôÔ∏è *Runner*: Ejecuta la concatenaci√≥n y devuelve un *output*.
4. üîÅ *Iterator*: Itera *N* veces o hasta que se cumpla una condici√≥n.
5. üíæ *Persister*: Guarda el resultado de cada iteraci√≥n en un archivo.
6. üí¨ *Context*: Guarda el contexto de la ejecuci√≥n previa para enviarla como feedback en la siguiente.


### Pseudo-c√≥digo

```shell
Subsystem.genCode(specs, feedback?) ‚Üí (GeneratedCode, Stdout/Stderr)
  ‚Üí LLM.send(specs + feedback) ‚Üí GeneratedCode
  ‚Üí Concatenator.concatenate(GeneratedCode, Specs) ‚Üí Concatenated
  ‚Üí SwiftRunner.run(Concatenated) ‚Üí Stdout/Stderr
  ‚Üí Exit

Coordinator.genCode(inputURL, outputURL, maxIterations)
  ‚Üí Context.save(IterationResult)
  ‚Üí Iterator.iterate(maxIterations, Subsystem.genCode)
  ‚Üí Persister.save(outputURL, IterationResult)
```

Iterator encapsula la l√≥gica de iteraci√≥n y devueve el √∫ltimo resultado:

```swift
iterate<T>(
  nTimes: Int,
  action: () async -> T,
  until: (T) -> Void
) async -> T
```

Y permite romperla a trav√©s de un *closure*:

```swift
while currentIteration < nTimes {
  let result = await action()
  if until(result) { ... return and break ...}
}
```

El mismo closure puede ser utilizado para guardar el contexto:

```swift
let context = ContextBuilder(window: 5)
let messages = makeMessages(context, sysPrompt, specs)
iterator.iterate(
        nTimes,
        action: { generateCode(messages) },
        onIteration: { context.insert($0)}
)
```

Por simplicidad, en el *playground* del art√≠culo, el contexto contiene s√≥lo el resultado de la iteraci√≥n anterior

### Contratos

Para hacer el proyecto flexible y *testeable*,  los actores son modelados con contratos en lugar de implementaciones concretas:

```swift
protocol Client {
    func send(messages: [Message]) async throws -> String
}

protocol Runner {
    func run(code: String) -> String
}

protocol Perister {
    func persist(code: String, to outputURL: String) throws
}

protocol Reader {
    func read(_ inputURL: String) throws -> String
}
```

Gracias a este enfoque, podemos a√±adir nuevos modelos, runners alternativos o incluso sistemas de almacenamiento sin tocar la l√≥gica principal del programa. Esto tambi√©n simplifica las pruebas, porque cada componente puede ser mockeado por separado.


## Data

La primera versi√≥n del proyecto fue muy sencilla: Unos pocos ficheros de *Swift* compilados con *CodeRunner*.
Con ella hice pruebas de intensidad moderada (tanto de especificaciones como de modelos).

Desafortunadamente, no tengo acceso a esos datos.

A falta de datos, solo puedo decir que el modelo que peor se port√≥ fue *Gemini*.

Los que mejor desempe√±o tuvieron fueron *Claude* y *ChatGPT*.

*Llama 3.2* di√≥ resultados variables, aunque la velocidad de iteraci√≥n por ser una ejecuci√≥n local compensaba muchas veces las carencias.

A√∫n con la p√©rdida del proyecto original, conservo algunos resultados de codestral:

{{< runnable "./results/codestral/FileImporter.swift.html">}}
{{< runnable "./results/codestral/LineAppender.swift.html">}}
{{< runnable "./results/codestral/PasswordGenerator.swift.html">}}


## Problemas

No he tenido la oportunidad de probar  este enfoque tan exhaustivamente como me gustar√≠a, pero pude recopilar algunos ejemplos de problem√°ticas que me encontr√© en el camino.

### Cuando Codestral te da una palmadita y te dice: ‚Äúte dejo el resto como ejercicio, campe√≥n‚Äù

Partiendo de estas *specs*:

```swift
func test_fetch_reposWithMinimumStarsFromRealApi() async throws {
  let sut = GithubClient()
  // This MUST PERFORM A REAL CALL TO THE GITHUB API
  let repos = try await sut.fetchRepositories(minStars: 100)
  assert(!repos.isEmpty)
  assert(repos.allSatisfy { $0.stars >= 100 })
}
```

*Codestral* fue capaz de generar un cliente **funcional**, a pesar de algunas dificultades iniciales:

```swift
struct Repository: Decodable {
  let name: String
  let stargazers_count: Int
  var stars: Int { stargazers_count }
}

class GithubClient {
  func fetchRepositories(minStars: Int) async throws -> [Repository] {
    let url = URL(string: "https://api.github.com/search/repositories?q=stars:>\(minStars)&sort=stars")!
    let (data, _) = try await URLSession.shared.data(from: url)
    let results = try JSONDecoder().decode(SearchResults<Repository>.self, from: data)
    return results.items
  }
}

struct SearchResults<T: Decodable>: Decodable {
  let items: [T]
}
```

Pero en un inicio el modelo insist√≠a en generarme c√≥digo de este tipo:

```swift
class GithubClient {
  func fetchRepositories(minStars: Int) async throws -> [Repository] {
  /* YOUR IMPLEMENTATION HERE */
  return []
  }
}
```

~~Gracias, Codestral. Con eso y un croquis, casi tengo un sistema distribuido.~~

### Cuando el modelo no resuelve el problema... porque ya sabe la respuesta

Aunque poco frecuente, otro caso que me encontr√© ocasionalmente, fue el de pruebas satisfechas *"en duro"*. Ej:

```swift
func test_adder() {
  let sut = Adder(1,3)
  assert(sut.result == 4)
}
```

El modelo generaba esto:

```swift
struct Adder {
  let result = 4
  init (_ a: Int, _ b: Int) {}
}
```

Estos casos se solucionan f√°cilmente a√±adiendo m√°s aserciones a la prueba:

```swift
func test_adder() {
  var sut = Adder(1,3)
  assert(sut.result == 4)

  sut = Adder(3, 4)
  assert(sut.result == 7)

  sut = Adder(5, 4)
  assert(sut.result == 9)
}
```

### Cuando *Gemini* quiere ser tu profe, pero t√∫ solo quieres compilar

En el *system prompt* que definimos, el siguiente apartado es importante para que el c√≥digo pueda compilar correctamente:

> Provide ONLY runnable Swift code. No explanations, comments, or formatting (no code blocks, markdown, symbols, or text).

A√∫n con este *prompt*, algunos modelos (*Gemini*...), ten√≠an dificultades respetando las instrucciones y se empe√±aban en encapsular el c√≥digo en bloques de c√≥digo de markdown, acompa√±√°ndolo adem√°s de comentarios explicativos.

Aunque se agradece el entusiasmo por la pedagog√≠a, hubiera preferido no tener que escribir una funci√≥n de preprocesamiento para limpiar los artefactos de las respuestas.

## Limitaciones

Esta idea asume especificaciones completamente ajustadas al sistema de de antemano, algo poco realista.

Tambi√©n asume que las especificaciones no tienen ning√∫n error. Cosa que es tambi√©n poco realista.

Al desarrollar, y en especial en el enfoque *test driven*, es com√∫n que las especificaciones emerjan de forma org√°nica durante el desarrollo: El proceso en s√≠ es un *marco* para el pensamiento.

Muchas veces escribimos tests que refactorizamos o eliminamos a medida que vamos aprendiendo sobre el sistema.

Creo que la idea puede ser especialmente √∫til para tareas automatizables o problemas repetitivos, pero no para problemas complejos en d√≥nde el proceso de desarrollo acompa√±a la definici√≥n o refinamiento de las especificaciones.

Por ejemplo, en vez de utilizar la idea para implementaciones de Infrastructure, podr√≠a aprovecharse para generar implementaciones de coordinadores a partir de situaciones recurrentes repetitivas en *TDD*, como verificar que un sistema y sus dependencias interact√∫an correctamente mediante mocks:

```swift
// Failure cases
func test_coordinator_deliversErrorOnClientError() async {}
func test_coordinator_deliversErrorOnRunnerError() async {}
func test_coordinator_deliversErrorOnPersisterError() async {}
...
// Success cases
```

## Conclusiones

Aunque este experimento tiene limitaciones claras ‚Äîcomo la dependencia de especificaciones precisas y la falta de pruebas exhaustivas‚Äî, me parece un enfoque prometedor.

Automatizar el ciclo de prueba y correcci√≥n puede liberar tiempo para tareas m√°s relevantes del desarrollo, siempre que el problema est√© bien acotado. En ese sentido, este tipo de sistema podr√≠a resultar especialmente √∫til para tareas repetitivas o muy estructuradas.

El reto real no est√° en la capacidad t√©cnica del modelo, sino en c√≥mo integrar estas herramientas en el flujo de trabajo diario sin a√±adir m√°s fricci√≥n. Dir√≠a que no es un problema de potencia, porque a√∫n con las limitaciones de los modelos actuales, el enfoque puede ser √∫til, sino de experiencia de usuario.

## Ideas futuras

Quedan muchas cosas por explorar. Esta primera versi√≥n fue una prueba de concepto centrada en el flujo m√°s simple posible, pero hay espacio para hacer el sistema m√°s robusto, flexible y √∫til en contextos reales.

Algunas direcciones que me interesar√≠a explorar:
- Integrar un framework de pruebas real.
- Generar autom√°ticamente tests para estructuras comunes con mocking.
- Utilizar snapshots como fuente de especificaci√≥n y validar la salida del modelo con aserciones snapshot.
- Ejecutar peticiones paralelas con varios modelos y romper la iteraci√≥n en cuanto uno pase la prueba.
- Ajustar din√°micamente el prompt en funci√≥n de fallos consecutivos, usando otro modelo como refinador.
- A√±adir un sistema de notificaciones al finalizar las pruebas.

## Enlaces

1. [LLM7](llm7.io)
2. [C√≥digo fuente del playground](https://github.com/crisfeim/crisfe.im/tree/main/content/posts/2025.05.13.making-the-ai-suffer-so-you-dont-have-to/codegen-demo)

[^1]: Aunque no original: [cf.github](https://github.com/crisfeim/cli-tddbuddy/search?q=tdd&type=code).
[^2]: *XCTest* / *Swift Testing*
[^8]: El compilador no acepta un *string* como entrada.
